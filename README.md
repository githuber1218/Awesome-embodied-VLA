# Awesome-embodied-VLA

This repo contains a curative list of **papers using VLM/VLA**. This repo is still

---
## Overview
  - [Manipulation](#Manipulation)
  - [Dataset](#Dataset)
  - [Citation](#Citation)

---
## Embodied
* **VoxPoser**: "Voxposer: Composable 3D Value Maps for Robotic Manipulation with Language Models", *arXiv, Nov 2023*. [[Paper](https://voxposer.github.io/voxposer.pdf)] [[Code](https://github.com/huangwl18/VoxPoser)] [[Website](https://voxposer.github.io/)]

* **ReKep**: "ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation", *arXiv, Sep 2024*. [[Paper](https://rekep-robot.github.io/rekep.pdf)] [[Code](https://github.com/huangwl18/ReKep)] [[Website](https://rekep-robot.github.io)]

* **RoboNurse-VLA**: "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action", *arXiv, Sep 2024*. [[Paper](https://arxiv.org/abs/2409.19590)][[Website](https://arxiv.org/abs/2409.19590)]

* **TinyVLA**: "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation", *arXiv, Sep 2024*. [[Paper](https://arxiv.org/pdf/2409.12514)]

* **EdgeVLA**: "EdgeVLA: Efficient Vision-Language-Action Models", *Nov 2024*. [[Paper](https://kscale-public.s3.amazonaws.com/evla_09092024/report.pdf)]

* **OpenVLA**: "OpenVLA: An Open-Source Vision-Language-Action Model", *arXiv, June 2024*. [[Paper](https://arxiv.org/abs/2406.09246)][[Website](https://openvla.github.io/)]

* **ECoT**: "Robotic Control via Embodied Chain-of-Thought Reasoning", *arXiv, Jul 2024*. [[Paper](https://arxiv.org/abs/2407.08693)]
---
## Simulation
* **HumanVLA**: "HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid", *arXiv, Nov 2024*. [[Paper](https://arxiv.org/pdf/2406.19972)][[Code](https://github.com/AllenXuuu/HumanVLA)]
---
## Dataset
* **BridgeData V2**:"BridgeData V2: A Dataset for Robot Learning at Scale", *arXiv, Aug 2023*. [[Paper](https://arxiv.org/abs/2308.12952)][[Website](https://rail-berkeley.github.io/bridgedata/)][[Code](https://github.com/rail-berkeley/bridge_data_v2)]
* **Open X-Embodiment Dataset**:"Open X-Embodiment: Robotic Learning Datasets and RT-X Models", *arXiv, Oct 2023*. [[Paper](https://arxiv.org/abs/2310.08864)][[Website](https://robotics-transformer-x.github.io/)]
----
## Citation
If you find this repository useful, please consider citing this list:
```
@misc{weihengwang2024vlapaperslist,
    title = {Awesome-embodied-VLA},
    author = {Weiheng Wang},
    journal = {GitHub repository},
    url = {https://github.com/githuber1218/Awesome-embodied-VLA},
    year = {2024},
}
```
